{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/weave/blob/anish/codegen-testing-coobook/examples/cookbooks/codegen/codegen.ipynb)\n",
    "\n",
    "<!--- @wandbcode{codegen-cookbook} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU autopep8 autoflake weave isort openai set-env-colab-kaggle-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import Any, Dict\n",
    "\n",
    "import autopep8\n",
    "import isort\n",
    "import weave\n",
    "from autoflake import fix_code\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from weave import Dataset, Evaluation\n",
    "\n",
    "from set_env import set_env\n",
    "\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "set_env(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAVE_PROJECT = \"codegen-cookbook\"\n",
    "weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = Dataset(name=\"minimal_code_gen_example\", rows=[\n",
    "    {\n",
    "        \"prompt\": \"Create a Python function that calculates the factorial of a given number.\"\n",
    "    }\n",
    "])\n",
    "weave.publish(prompt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedCode(BaseModel):\n",
    "    code: str\n",
    "\n",
    "class ProgramRunner(BaseModel):\n",
    "    main_function_code: str\n",
    "\n",
    "class UnitTest(BaseModel):\n",
    "    test_code: str\n",
    "\n",
    "class CodeFormatter(BaseModel):\n",
    "\n",
    "    @weave.op()\n",
    "    def lint_code(self, code: str) -> str:\n",
    "        # Replace escaped newlines with actual newlines\n",
    "        code = code.replace('\\\\n', '\\n')\n",
    "\n",
    "        # Remove unused imports and variables\n",
    "        code = fix_code(code, remove_all_unused_imports=True,\n",
    "                        remove_unused_variables=True)\n",
    "\n",
    "        # Sort imports\n",
    "        code = isort.code(code)\n",
    "\n",
    "        # Apply PEP 8 formatting\n",
    "        code = autopep8.fix_code(code, options={'aggressive': 1})\n",
    "\n",
    "        return code\n",
    "\n",
    "    @weave.op()\n",
    "    def format_generated_code(self, generated_code: GeneratedCode) -> GeneratedCode:\n",
    "        cleaned_code = self.lint_code(generated_code.code)\n",
    "        return GeneratedCode(code=cleaned_code)\n",
    "\n",
    "    @weave.op()\n",
    "    def format_program_runner(self, program_runner: ProgramRunner) -> ProgramRunner:\n",
    "        cleaned_code = self.lint_code(program_runner.main_function_code)\n",
    "        return ProgramRunner(main_function_code=cleaned_code)\n",
    "\n",
    "    @weave.op()\n",
    "    def format_unit_test(self, unit_test: UnitTest) -> UnitTest:\n",
    "        cleaned_code = self.lint_code(unit_test.test_code)\n",
    "        return UnitTest(test_code=cleaned_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerationPipeline(weave.Model):\n",
    "\n",
    "    model_name: str\n",
    "    formatter: CodeFormatter\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt-4o\", formatter: CodeFormatter = CodeFormatter()):\n",
    "        super().__init__(model_name=model_name, formatter=formatter)\n",
    "        self.model_name = model_name\n",
    "        self.formatter = formatter\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, prompt: str):\n",
    "        generated_code = self.generate_code(prompt)\n",
    "        formatted_generated_code = self.formatter.format_generated_code(generated_code)\n",
    "\n",
    "        program_runner = self.generate_program(formatted_generated_code)\n",
    "        formatted_program_runner = self.formatter.format_program_runner(program_runner)\n",
    "\n",
    "        unit_tests = self.generate_tests(formatted_generated_code, formatted_program_runner)\n",
    "        formatted_unit_tests = self.formatter.format_unit_test(unit_tests)\n",
    "        \n",
    "        return {\n",
    "            \"generated_code\": formatted_generated_code,\n",
    "            \"program_runner\": formatted_program_runner,\n",
    "            \"unit_tests\": formatted_unit_tests,\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_code(self, prompt: str) -> GeneratedCode:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert Python code generator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format=GeneratedCode,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_program(self, generated_code: GeneratedCode) -> ProgramRunner:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert Python program generator. Create a main function that orchestrates the execution of the given functions. Follow these guidelines:\n",
    "\n",
    "1. Create a main() function that calls the necessary functions to run the program.\n",
    "2. Include a proper if __name__ == \"__main__\": guard to call the main() function.\n",
    "3. Do not redefine or implement any functions; use only the functions provided.\n",
    "4. Do not include any imports or package specifications.\n",
    "5. Use clear and concise code with proper indentation.\n",
    "6. Do not use escape characters for newlines; write actual line breaks.\n",
    "7. Keep the main() function simple, calling only the top-level function(s) needed.\n",
    "\n",
    "Example structure:\n",
    "\n",
    "def main():\n",
    "    result = top_level_function()\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "Remember, your task is solely to create the main() function and the __main__ guard. All other functions are assumed to be already defined.\n",
    "\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate a main function for this code:\\n\\n{generated_code.code}\"}\n",
    "            ],\n",
    "            response_format=ProgramRunner,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_tests(self, generated_code: GeneratedCode, program_runner: ProgramRunner) -> UnitTest:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert Python unit test generator.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Generate a complete unittest for the following code:\n",
    "\n",
    "Context (Surrounding Code):\n",
    "```python\n",
    "{generated_code.code}\n",
    "{program_runner.main_function_code}\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. **Structure:** Use `unittest.TestCase` and name the class `Test<FunctionName>`.\n",
    "2. **Coverage:** Include tests for normal cases, edge cases, and potential errors.\n",
    "3. **Naming:** Use descriptive test method names (e.g., `test_valid_input`, `test_empty_input`, `test_invalid_input_type`).\n",
    "4. **Type Hints:** Include type hints for clarity.\n",
    "5. **Mocking:** Mock external dependencies (e.g., database interactions, API calls) when necessary.\n",
    "6. **Assertions:** Use appropriate assertions (e.g., `assertEqual`, `assertRaises`, `assertTrue`).\n",
    "7. **Isolation:** Ensure test isolation to prevent interference between tests.\n",
    "9. **Executable:** Include a `__main__` block to run tests directly: `if __name__ == '__main__': unittest.main()`\n",
    "10. **Formatting:** Ensure proper indentation and formatting for readability.\n",
    "11. **Imports:** Include all necessary imports.\n",
    "12. **Completeness:** Provide a complete, runnable test file.\n",
    "\n",
    "Provide only the complete, properly formatted test code, no explanations or markdown.\n",
    "\"\"\"}\n",
    "            ],\n",
    "            response_format=UnitTest,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestResultScorer(weave.Scorer):\n",
    "    @weave.op()\n",
    "    def score(self, model_output: Dict[str, Any], prompt: str) -> Dict[str, Any]:\n",
    "        if not model_output or \"generated_code\" not in model_output:\n",
    "            return {\"error\": \"No generated code provided\"}\n",
    "\n",
    "        generated_code = model_output[\"generated_code\"].code\n",
    "        unit_tests = model_output[\"unit_tests\"].test_code\n",
    "\n",
    "        code_quality_score = self.assess_code_quality(generated_code)\n",
    "        test_coverage_score = self.assess_test_coverage(generated_code, unit_tests)\n",
    "\n",
    "        overall_score = (code_quality_score + test_coverage_score) / 2\n",
    "\n",
    "        return {\n",
    "            \"code_quality_score\": code_quality_score,\n",
    "            \"test_coverage_score\": test_coverage_score,\n",
    "            \"overall_score\": overall_score\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    def assess_code_quality(self, code: str) -> float:\n",
    "        score = 0.0\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Check for docstrings\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):\n",
    "                    if ast.get_docstring(node):\n",
    "                        score += 0.2\n",
    "\n",
    "            # Check for type hints\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    if node.returns or any(arg.annotation for arg in node.args.args):\n",
    "                        score += 0.2\n",
    "\n",
    "            # Check for meaningful variable names\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.Name):\n",
    "                    if len(node.id) > 1 and not node.id.startswith('_'):\n",
    "                        score += 0.1\n",
    "\n",
    "            # Penalize for excessive line length\n",
    "            lines = code.split('\\n')\n",
    "            if any(len(line) > 100 for line in lines):\n",
    "                score -= 0.2\n",
    "\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "        return min(max(score, 0.0), 1.0)\n",
    "\n",
    "    @weave.op()\n",
    "    def assess_test_coverage(self, code: str, unit_tests: str) -> float:\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check if unit tests are provided\n",
    "        if not unit_tests:\n",
    "            return 0.0\n",
    "\n",
    "        try:\n",
    "            code_tree = ast.parse(code)\n",
    "            test_tree = ast.parse(unit_tests)\n",
    "\n",
    "            code_functions = [node.name for node in ast.walk(code_tree) if isinstance(node, ast.FunctionDef)]\n",
    "            test_functions = [node.name for node in ast.walk(test_tree) if isinstance(node, ast.FunctionDef) and node.name.startswith('test_')]\n",
    "\n",
    "            # Score based on the number of test functions relative to code functions\n",
    "            coverage_ratio = len(test_functions) / len(code_functions) if code_functions else 0\n",
    "            score = min(coverage_ratio, 1.0)\n",
    "\n",
    "            # Bonus for using assertions\n",
    "            if 'self.assert' in unit_tests:\n",
    "                score += 0.2\n",
    "\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "        return min(score, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"gpt-4o-2024-08-06\"]:\n",
    "    pipeline = CodeGenerationPipeline(model_name=model_name)\n",
    "    test_result_scorer = TestResultScorer()\n",
    "    evaluation = Evaluation(\n",
    "        name=\"minimal_code_gen_evaluation\",\n",
    "        dataset=prompt_dataset,\n",
    "        scorers=[test_result_scorer]\n",
    "    )\n",
    "    results = await evaluation.evaluate(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

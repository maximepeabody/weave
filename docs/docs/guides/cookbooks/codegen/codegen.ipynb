{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import weave\n",
    "from weave import Dataset, Evaluation\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "import autopep8\n",
    "import isort\n",
    "from autoflake import fix_code\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEAVE_PROJECT = \"codegen-cookbook\"\n",
    "# weave.init(WEAVE_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dataset = Dataset(name=\"minimal_code_gen_example\", rows=[\n",
    "    {\n",
    "        \"prompt\": \"Create a Python function that calculates the factorial of a given number.\"\n",
    "    }\n",
    "])\n",
    "# weave.publish(prompt_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedCode(BaseModel):\n",
    "    code: str\n",
    "\n",
    "class ProgramRunner(BaseModel):\n",
    "    main_function_code: str\n",
    "\n",
    "class UnitTest(BaseModel):\n",
    "    test_code: str\n",
    "\n",
    "class CodeFormatter(BaseModel):\n",
    "\n",
    "    @weave.op()\n",
    "    def lint_code(self, code: str) -> str:\n",
    "        # Replace escaped newlines with actual newlines\n",
    "        code = code.replace('\\\\n', '\\n')\n",
    "\n",
    "        # Remove unused imports and variables\n",
    "        code = fix_code(code, remove_all_unused_imports=True,\n",
    "                        remove_unused_variables=True)\n",
    "\n",
    "        # Sort imports\n",
    "        code = isort.code(code)\n",
    "\n",
    "        # Apply PEP 8 formatting\n",
    "        code = autopep8.fix_code(code, options={'aggressive': 1})\n",
    "\n",
    "        return code\n",
    "\n",
    "    @weave.op()\n",
    "    def format_generated_code(self, generated_code: GeneratedCode) -> GeneratedCode:\n",
    "        cleaned_code = self.lint_code(generated_code.code)\n",
    "        return GeneratedCode(code=cleaned_code)\n",
    "\n",
    "    @weave.op()\n",
    "    def format_program_runner(self, program_runner: ProgramRunner) -> ProgramRunner:\n",
    "        cleaned_code = self.lint_code(program_runner.main_function_code)\n",
    "        return ProgramRunner(main_function_code=cleaned_code)\n",
    "\n",
    "    @weave.op()\n",
    "    def format_unit_test(self, unit_test: UnitTest) -> UnitTest:\n",
    "        cleaned_code = self.lint_code(unit_test.test_code)\n",
    "        return UnitTest(test_code=cleaned_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeGenerationPipeline(weave.Model):\n",
    "\n",
    "    model_name: str\n",
    "    formatter: CodeFormatter\n",
    "    client: OpenAI\n",
    "\n",
    "    def __init__(self, model_name: str = \"gpt-4o\", formatter: CodeFormatter = CodeFormatter(), client: OpenAI = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))):\n",
    "        super().__init__(model_name=model_name, formatter=formatter, client=client)\n",
    "        self.model_name = model_name\n",
    "        self.formatter = formatter\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, prompt: str):\n",
    "        generated_code = self.generate_code(prompt)\n",
    "        formatted_generated_code = self.formatter.format_generated_code(generated_code)\n",
    "\n",
    "        program_runner = self.generate_program(formatted_generated_code)\n",
    "        formatted_program_runner = self.formatter.format_program_runner(program_runner)\n",
    "\n",
    "        unit_tests = self.generate_tests(formatted_generated_code, formatted_program_runner)\n",
    "        formatted_unit_tests = self.formatter.format_unit_test(unit_tests)\n",
    "        \n",
    "        return {\n",
    "            \"generated_code\": formatted_generated_code,\n",
    "            \"program_runner\": formatted_program_runner,\n",
    "            \"unit_tests\": formatted_unit_tests,\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_code(self, prompt: str) -> GeneratedCode:\n",
    "        completion = self.client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert Python code generator.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format=GeneratedCode,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_program(self, generated_code: GeneratedCode) -> ProgramRunner:\n",
    "        completion = self.client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "You are an expert Python program generator. Create a main function that orchestrates the execution of the given functions. Follow these guidelines:\n",
    "\n",
    "1. Create a main() function that calls the necessary functions to run the program.\n",
    "2. Include a proper if __name__ == \"__main__\": guard to call the main() function.\n",
    "3. Do not redefine or implement any functions; use only the functions provided.\n",
    "4. Do not include any imports or package specifications.\n",
    "5. Use clear and concise code with proper indentation.\n",
    "6. Do not use escape characters for newlines; write actual line breaks.\n",
    "7. Keep the main() function simple, calling only the top-level function(s) needed.\n",
    "\n",
    "Example structure:\n",
    "\n",
    "def main():\n",
    "    result = top_level_function()\n",
    "    print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "Remember, your task is solely to create the main() function and the __main__ guard. All other functions are assumed to be already defined.\n",
    "\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Generate a main function for this code:\\n\\n{generated_code.code}\"}\n",
    "            ],\n",
    "            response_format=ProgramRunner,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)\n",
    "\n",
    "    @weave.op()\n",
    "    def generate_tests(self, generated_code: GeneratedCode, program_runner: ProgramRunner) -> UnitTest:\n",
    "        completion = self.client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert Python unit test generator.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Generate a complete unittest for the following code:\n",
    "\n",
    "Context (Surrounding Code):\n",
    "```python\n",
    "{generated_code.code}\n",
    "{program_runner.main_function_code}\n",
    "```\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. **Structure:** Use `unittest.TestCase` and name the class `Test<FunctionName>`.\n",
    "2. **Coverage:** Include tests for normal cases, edge cases, and potential errors.\n",
    "3. **Naming:** Use descriptive test method names (e.g., `test_valid_input`, `test_empty_input`, `test_invalid_input_type`).\n",
    "4. **Type Hints:** Include type hints for clarity.\n",
    "5. **Mocking:** Mock external dependencies (e.g., database interactions, API calls) when necessary.\n",
    "6. **Assertions:** Use appropriate assertions (e.g., `assertEqual`, `assertRaises`, `assertTrue`).\n",
    "7. **Isolation:** Ensure test isolation to prevent interference between tests.\n",
    "9. **Executable:** Include a `__main__` block to run tests directly: `if __name__ == '__main__': unittest.main()`\n",
    "10. **Formatting:** Ensure proper indentation and formatting for readability.\n",
    "11. **Imports:** Include all necessary imports.\n",
    "12. **Completeness:** Provide a complete, runnable test file.\n",
    "\n",
    "Provide only the complete, properly formatted test code, no explanations or markdown.\n",
    "\"\"\"}\n",
    "            ],\n",
    "            response_format=UnitTest,\n",
    "        )\n",
    "        message = completion.choices[0].message\n",
    "        if message.parsed:\n",
    "            return message.parsed\n",
    "        else:\n",
    "            raise ValueError(message.refusal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestResultScorer(weave.Scorer):\n",
    "    @weave.op()\n",
    "    def score(self, model_output: Dict[str, Any], prompt: str) -> Dict[str, Any]:\n",
    "        if not model_output or \"generated_code\" not in model_output:\n",
    "            return {\"error\": \"No generated code provided\"}\n",
    "\n",
    "        generated_code = model_output[\"generated_code\"].code\n",
    "        unit_tests = model_output[\"unit_tests\"].test_code\n",
    "\n",
    "        code_quality_score = self.assess_code_quality(generated_code)\n",
    "        test_coverage_score = self.assess_test_coverage(generated_code, unit_tests)\n",
    "        functionality_score = self.assess_functionality(generated_code, prompt)\n",
    "\n",
    "        overall_score = (code_quality_score + test_coverage_score + functionality_score) / 3\n",
    "\n",
    "        return {\n",
    "            \"code_quality_score\": code_quality_score,\n",
    "            \"test_coverage_score\": test_coverage_score,\n",
    "            \"functionality_score\": functionality_score,\n",
    "            \"overall_score\": overall_score\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    def assess_code_quality(self, code: str) -> float:\n",
    "        score = 0.0\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            \n",
    "            # Check for docstrings\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.Module)):\n",
    "                    if ast.get_docstring(node):\n",
    "                        score += 0.2\n",
    "\n",
    "            # Check for type hints\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    if node.returns or any(arg.annotation for arg in node.args.args):\n",
    "                        score += 0.2\n",
    "\n",
    "            # Check for meaningful variable names\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.Name):\n",
    "                    if len(node.id) > 1 and not node.id.startswith('_'):\n",
    "                        score += 0.1\n",
    "\n",
    "            # Penalize for excessive line length\n",
    "            lines = code.split('\\n')\n",
    "            if any(len(line) > 100 for line in lines):\n",
    "                score -= 0.2\n",
    "\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "        return min(max(score, 0.0), 1.0)\n",
    "\n",
    "    @weave.op()\n",
    "    def assess_test_coverage(self, code: str, unit_tests: str) -> float:\n",
    "        score = 0.0\n",
    "        \n",
    "        # Check if unit tests are provided\n",
    "        if not unit_tests:\n",
    "            return 0.0\n",
    "\n",
    "        try:\n",
    "            code_tree = ast.parse(code)\n",
    "            test_tree = ast.parse(unit_tests)\n",
    "\n",
    "            code_functions = [node.name for node in ast.walk(code_tree) if isinstance(node, ast.FunctionDef)]\n",
    "            test_functions = [node.name for node in ast.walk(test_tree) if isinstance(node, ast.FunctionDef) and node.name.startswith('test_')]\n",
    "\n",
    "            # Score based on the number of test functions relative to code functions\n",
    "            coverage_ratio = len(test_functions) / len(code_functions) if code_functions else 0\n",
    "            score = min(coverage_ratio, 1.0)\n",
    "\n",
    "            # Bonus for using assertions\n",
    "            if 'self.assert' in unit_tests:\n",
    "                score += 0.2\n",
    "\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    @weave.op()\n",
    "    def assess_functionality(self, code: str, prompt: str) -> float:\n",
    "        score = 0.0\n",
    "\n",
    "        # Check if the code addresses the main points in the prompt\n",
    "        prompt_keywords = set(re.findall(r'\\b\\w+\\b', prompt.lower()))\n",
    "        code_keywords = set(re.findall(r'\\b\\w+\\b', code.lower()))\n",
    "\n",
    "        keyword_match_ratio = len(prompt_keywords.intersection(code_keywords)) / len(prompt_keywords)\n",
    "        score += keyword_match_ratio\n",
    "\n",
    "        # Check if the code contains expected elements based on the prompt\n",
    "        try:\n",
    "            tree = ast.parse(code)\n",
    "            if 'calculate' in prompt.lower() and any(isinstance(node, ast.Return) for node in ast.walk(tree)):\n",
    "                score += 0.3\n",
    "\n",
    "            if 'function' in prompt.lower() and any(isinstance(node, ast.FunctionDef) for node in ast.walk(tree)):\n",
    "                score += 0.3\n",
    "        except SyntaxError:\n",
    "            return 0.0\n",
    "\n",
    "        return min(score, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">model_output failed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "model_output failed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/weave/flow/eval.py\", line 164, in predict_and_score\n",
      "    model_output = await async_call(model_predict, **model_predict_args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/weave/trace/op.py\", line 326, in wrapper\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7l/0d0kns_n6f19hvbjw81shpkh0000gp/T/ipykernel_73119/4120024944.py\", line 15, in predict\n",
      "    generated_code = self.generate_code(prompt)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/weave/trace/op.py\", line 335, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/7l/0d0kns_n6f19hvbjw81shpkh0000gp/T/ipykernel_73119/4120024944.py\", line 32, in generate_code\n",
      "    completion = self.client.beta.chat.completions.parse(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/openai/resources/beta/chat/completions.py\", line 112, in parse\n",
      "    raw_completion = self._client.chat.completions.create(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 650, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 936, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/Users/anishshah/Documents/GitHub/weave/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1040, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid parameter: 'response_format' must be one of 'json_object', 'text'.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'TestResultScorer'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.39632105827331543</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\u001b[32m'TestResultScorer'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.39632105827331543\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TestResultScorer': None, 'model_latency': {'mean': 0.39632105827331543}}\n"
     ]
    }
   ],
   "source": [
    "for model_name in [\"gpt-4o\"]:\n",
    "    pipeline = CodeGenerationPipeline(model_name=model_name)\n",
    "    test_result_scorer = TestResultScorer()\n",
    "    evaluation = Evaluation(\n",
    "        name=\"minimal_code_gen_evaluation\",\n",
    "        dataset=prompt_dataset,\n",
    "        scorers=[test_result_scorer]\n",
    "    )\n",
    "    results = await evaluation.evaluate(pipeline)\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

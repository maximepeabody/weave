{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- docusaurus_head_meta::start\n",
        "---\n",
        "title: PII Cookbook Presidio\n",
        "---\n",
        "docusaurus_head_meta::end -->\n",
        "\n",
        "<img src=\"http://wandb.me/logo-im-png\" width=\"400\" alt=\"Weights & Biases\" />\n",
        "<!--- @wandbcode{cod-notebook} -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m752k2fWKDql"
      },
      "source": [
        "# How to use Weave with PII data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70egOGRLCgm"
      },
      "source": [
        "In this tutorial, we'll demonstrate how to utilize Weave while preventing your Personally Identifiable Information (PII) data from being incorporated into Weave or the LLMs you employ.\n",
        "\n",
        "To protect our PII data, we'll employ Microsoft's [Presidio](https://microsoft.github.io/presidio/), a Python-based data protection SDK. This tool provides redaction and replacement functionalities, both of which we will implement in this tutorial.\n",
        "\n",
        "For this use-case. We will leverage Anthropic's claude-3-sonnet to perform sentiment analysis. While we use Weave's [Traces](https://wandb.github.io/weave/quickstart) to track and analize the LLM's API calls. Sonnet will receive block of text and output one of the following sentiment classification:\n",
        "1. positive\n",
        "2. negative\n",
        "3. neutral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qi-VNJT35v2j",
        "outputId": "8870f124-e141-4fee-8789-024d81944c56"
      },
      "outputs": [],
      "source": [
        "# @title required python packages:\n",
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!python -m spacy download en_core_web_lg # Presidio uses spacy NLP engine\n",
        "!pip install Faker                       # we'll use Faker to replace PII data with fake data\n",
        "!pip install weave                        # To leverage Traces\n",
        "!pip install set-env-colab-kaggle-dotenv -q # for env var\n",
        "!pip install anthropic                      # to use sonnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYEX_yPqMDOk"
      },
      "source": [
        "# Method 1A:\n",
        "Our first method involves complete removal of PII data using Presidio. This approach redacts PII and replaces it with a placeholder representing the PII type. For example:\n",
        "```\n",
        " \"My name is Alex\"\n",
        "```\n",
        "\n",
        "Will be:\n",
        "\n",
        "```\n",
        " \"My name is <PERSON>\"\n",
        "```\n",
        "Presidio comes with a built-in [list of recognizable entities](https://microsoft.github.io/presidio/supported_entities/). We can select the ones that are important for our use case. In the below example, we are only looking at redicating names and phone numbers from our text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh6MgL3g6sc2"
      },
      "outputs": [],
      "source": [
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "text= \"My phone number is 212-555-5555 and my name is alex\"\n",
        "\n",
        "# Set up the engine, loads the NLP module (spaCy model by default)\n",
        "# and other PII recognizers\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Call analyzer to get results\n",
        "results = analyzer.analyze(text=text,\n",
        "                           entities=[\"PHONE_NUMBER\", \"PERSON\"],\n",
        "                           language='en')\n",
        "\n",
        "# Analyzer results are passed to the AnonymizerEngine for anonymization\n",
        "\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "anonymized_text = anonymizer.anonymize(text=text,analyzer_results=results)\n",
        "\n",
        "print(anonymized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq1abYAUl4rV"
      },
      "source": [
        "Let's encapsulate the previous step into a function and expand the entity recognition capabilities. We will expand our redaction scope to include addresses, email addresses, and US Social Security numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rQyUmN5nmQrj",
        "outputId": "4a538fda-c079-4cea-a65f-ccbeee58f17e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        }
      ],
      "source": [
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "\"\"\"\n",
        "The below function will take a block of text, process it using presidio\n",
        "and return a block of text with the PII data redicated.\n",
        "PII data to be redicated:\n",
        "- Phone Numbers\n",
        "- Names\n",
        "- Addresses\n",
        "- Email addresses\n",
        "- US Social Security Numbers\n",
        "\"\"\"\n",
        "def anonymize_my_text(text):\n",
        "  results = analyzer.analyze(text=text,\n",
        "                           entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\",\"US_SSN\"],\n",
        "                           language='en')\n",
        "  anonymized_text = anonymizer.anonymize(text=text,analyzer_results=results)\n",
        "  return anonymized_text.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehhx3gK_teZQ"
      },
      "source": [
        "**Now we start building our tracing using Weave's Traces:**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s8ObFo3ngfI"
      },
      "source": [
        "Let's load our initial PII data. For demonstration purposes, we'll use a dataset containing 10 text blocks. A larger dataset with 1000 entries is available at [link].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1hyDUVTZsOUb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('10_pii_data.json') as f:\n",
        "    pii_data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsBADqp2u7-S"
      },
      "outputs": [],
      "source": [
        "# @title Make sure to set up set up your API keys correctly\n",
        "from set_env import set_env\n",
        "_ = set_env(\"ANTHROPIC_API_KEY\")\n",
        "_ = set_env(\"WANDB_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7D0ZBtiAvD8"
      },
      "source": [
        "In this example, we'll create a [Weave Model](https://wandb.github.io/weave/guides/core-types/models) which is a combination of data (which can include configuration, trained model weights, or other information) and code that defines how the model operates.\n",
        "In this model, we will include our predict function where the Anthropic API will be called.\n",
        "\n",
        "Once you run this code you will receive a link to the Weave project page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTGbemoMtcO8"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "import asyncio\n",
        "from anthropic import AsyncAnthropic\n",
        "\n",
        "# Weave model / predict function\n",
        "class sentiment_analysis_model(weave.Model):\n",
        "    model_name: str\n",
        "    system_prompt: str\n",
        "    temperature: int\n",
        "\n",
        "    @weave.op()\n",
        "    async def predict(self, text_block: str) -> dict:\n",
        "        client =AsyncAnthropic()\n",
        "\n",
        "        response = await client.messages.create(\n",
        "            max_tokens=1024,\n",
        "            model=self.model_name,\n",
        "            system=self.system_prompt,\n",
        "            messages=[\n",
        "                {   \"role\": \"user\",\n",
        "                    \"content\":[\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": text_block\n",
        "                        }\n",
        "                        ]\n",
        "                 }\n",
        "            ]\n",
        "        )\n",
        "        result = response.content[0].text\n",
        "        if result is None:\n",
        "            raise ValueError(\"No response from model\")\n",
        "        parsed = json.loads(result)\n",
        "        return parsed\n",
        "\n",
        "\n",
        "weave.init('sentiment_analysis-example')\n",
        "# create our LLM model with a system prompt\n",
        "model = sentiment_analysis_model(name=\"claude-3-sonnet\",\n",
        "                      model_name=\"claude-3-5-sonnet-20240620\",\n",
        "                      system_prompt=\"You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]. Your answer should be one word in json format: {classification}\",\n",
        "                      temperature=0\n",
        "                      )\n",
        "# for every block of text, anonymized first and then predict\n",
        "for entry in pii_data:\n",
        "  anonymized_entry = anonymize_my_text(entry[\"text\"])\n",
        "  (await model.predict(anonymized_entry))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7omtvsdpMuel"
      },
      "source": [
        "# Method 1B: Replace PII data with fake data\n",
        "\n",
        "Instead of redacting text, we can anonymize it by swapping PII (like names and phone numbers) with fake data generated using the [Faker](https://faker.readthedocs.io/en/master/) Python library. For example:\n",
        "\n",
        "\n",
        "```\n",
        "\"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
        "```\n",
        "Will be:\n",
        "\n",
        "\n",
        "```\n",
        "\"My name is Katherine Dixon and I like to fish. My phone number is 667.431.7379\"\n",
        "\n",
        "```\n",
        "\n",
        "To effectively utilize Presidio, we must supply references to our custom operators. These operators will direct Presidio to the functions responsible for swapping PII with fake data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XJa7u5T_WYd"
      },
      "outputs": [],
      "source": [
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig, EngineResult, RecognizerResult\n",
        "from faker import Faker\n",
        "\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# Create faker functions (note that it has to receive a value)\n",
        "def fake_name(x):\n",
        "    return fake.name()\n",
        "\n",
        "def fake_number(x):\n",
        "    return fake.phone_number()\n",
        "\n",
        "\n",
        "# Create custom operator for the PERSON and PHONE_NUMBER\" entities\n",
        "operators = {\n",
        "    \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
        "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
        "             }\n",
        "\n",
        "\n",
        "text_to_anonymize = \"My name is Raphael and I like to fish. My phone number is 212-555-5555\"\n",
        "\n",
        "# Analyzer output\n",
        "analyzer_results = analyzer.analyze(text=text_to_anonymize,\n",
        "                           entities=[\"PHONE_NUMBER\", \"PERSON\"],\n",
        "                           language='en')\n",
        "\n",
        "\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "# do not forget to pass the operators from above to the anonymizer\n",
        "anonymized_results = anonymizer.anonymize(\n",
        "    text=text_to_anonymize, analyzer_results=analyzer_results, operators=operators\n",
        ")\n",
        "\n",
        "print(anonymized_results.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4HJcskpSYdL"
      },
      "source": [
        "Let's consolidate our code into a single class and expand the list of entities to include the additional ones we identified earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yIaharNAWGh5"
      },
      "outputs": [],
      "source": [
        "from presidio_anonymizer import AnonymizerEngine\n",
        "from presidio_anonymizer.entities import OperatorConfig, EngineResult, RecognizerResult\n",
        "from faker import Faker\n",
        "import weave\n",
        "import asyncio\n",
        "from anthropic import AsyncAnthropic\n",
        "\n",
        "# Let's build a custom class for generating fake data that will extend Faker\n",
        "class my_faker(Faker):\n",
        "\n",
        "  # Create faker functions (note that it has to receive a value)\n",
        "  def fake_address(x):\n",
        "    return fake.address()\n",
        "\n",
        "  def fake_ssn(x):\n",
        "    return fake.ssn()\n",
        "\n",
        "  def fake_name(x):\n",
        "    return fake.name()\n",
        "\n",
        "  def fake_number(x):\n",
        "    return fake.phone_number()\n",
        "\n",
        "  def fake_email(x):\n",
        "    return fake.email()\n",
        "\n",
        "  # Create custom operators for the entities\n",
        "  operators = {\n",
        "    \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": fake_name}),\n",
        "    \"PHONE_NUMBER\": OperatorConfig(\"custom\", {\"lambda\": fake_number}),\n",
        "    \"EMAIL_ADDRESS\": OperatorConfig(\"custom\", {\"lambda\": fake_email}),\n",
        "    \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": fake_address}),\n",
        "    \"US_SSN\":OperatorConfig(\"custom\", {\"lambda\": fake_ssn})\n",
        "             }\n",
        "\n",
        "  def anonymize_my_text(self, text):\n",
        "    anonymizer = AnonymizerEngine()\n",
        "    analyzer_results = analyzer.analyze(text=text,\n",
        "                           entities=[\"PHONE_NUMBER\", \"PERSON\", \"LOCATION\", \"EMAIL_ADDRESS\", \"US_SSN\"],\n",
        "                           language='en')\n",
        "    anonymized_results = anonymizer.anonymize(text=text,\n",
        "                            analyzer_results=analyzer_results, operators=self.operators)\n",
        "    return anonymized_results.text\n",
        "\n",
        "\n",
        "# Weave model / predict function\n",
        "class sentiment_analysis_model(weave.Model):\n",
        "    model_name: str\n",
        "    system_prompt: str\n",
        "    temperature: int\n",
        "\n",
        "    @weave.op()\n",
        "    async def predict(self, text_block: str) -> dict:\n",
        "        client = AsyncAnthropic()\n",
        "\n",
        "        response = await client.messages.create(\n",
        "            max_tokens=1024,\n",
        "            model=self.model_name,\n",
        "            system=self.system_prompt,\n",
        "            messages=[\n",
        "                {   \"role\": \"user\",\n",
        "                    \"content\":[\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": text_block\n",
        "                        }\n",
        "                        ]\n",
        "                 }\n",
        "            ]\n",
        "        )\n",
        "        result = response.content[0].text\n",
        "        if result is None:\n",
        "            raise ValueError(\"No response from model\")\n",
        "        parsed = json.loads(result)\n",
        "        return parsed\n",
        "\n",
        "\n",
        "### Start our fun work here ###\n",
        "weave.init('sentiment_analysis-example')\n",
        "# create our LLM model with the system prompt\n",
        "model = sentiment_analysis_model(name=\"claude-3-sonnet\",\n",
        "                      model_name=\"claude-3-5-sonnet-20240620\",\n",
        "                      system_prompt=\"You are a Sentiment Analysis classifier. You will be classifying text based on their sentiment. Your input will be a block of text. You will answer with one the following rating option[\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"]. Your answer should one word in json format dict where the key is classification.\",\n",
        "                      temperature=0\n",
        "                      )\n",
        "faker = my_faker()\n",
        "for entry in pii_data:\n",
        "  anonymized_entry = faker.anonymize_my_text(entry[\"text\"])\n",
        "  (await model.predict(anonymized_entry))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHltGLxARdKC"
      },
      "source": [
        "*Optional Step*: For data traceability, add a hashing function after the anonymization process. This hash can be used to link the anonymized data back to its original form.\n",
        "\n",
        "***Happy tracing!***"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XiRkYiiIj2KX"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
